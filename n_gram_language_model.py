# -*- coding: utf-8 -*-
"""N_gram_language_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18y1w7SxS-ixVOYmQaaN1ARdrmbA7rW4S

This file builds up several traditional language models on corpus of Wikipedia articles.

WikiText-2, a corpus of high-quality Wikipedia articles. The dataset was originally introduced in the following paper: https://arxiv.org/pdf/1609.07843v1.pdf. A raw version of the data can easily be viewed here: https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2.

The code was primarily designed to run on Google Colab.
"""

!pip install torchdata==0.5.1
!pip install torchtext==0.14.0

"""## Preprocessing Data

* <b>Sentence splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp; We identify individual sentences by splitting paragraphs at punctuation tokens (".",  "!",  "?").

* <b>Sentence markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each sentence must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`).

* <b>Unknown words:</b>&nbsp;&nbsp;&nbsp;&nbsp;The WikiText dataset has replaced all works that are not in the corpora as (`<UNK>`)
"""

START = "<s>"   # Start-of-sentence token
END = "</s>"    # End-of-sentence-token
UNK = "<UNK>"   # Unknown word token

import torchtext
import random
import sys

def preprocess(data, vocab=None):
    final_data = []
    lowercase = "abcdefghijklmnopqrstuvwxyz"
    for paragraph in data:
        paragraph = [x if x != '<unk>' else UNK for x in paragraph.split()]
        if vocab is not None:
            paragraph = [x if x in vocab else UNK for x in paragraph]
        if paragraph == [] or paragraph.count('=') >= 2: continue
        sen = []
        prev_punct, prev_quot = False, False
        for word in paragraph:
            if prev_quot:
                if word[0] not in lowercase:
                    final_data.append(sen)
                    sen = []
                    prev_punct, prev_quot = False, False
            if prev_punct:
                if word == '"':
                    prev_punct, prev_quot = False, True
                else:
                    if word[0] not in lowercase:
                        final_data.append(sen)
                        sen = []
                        prev_punct, prev_quot = False, False
            if word in {'.', '?', '!'}: prev_punct = True
            sen += [word]
        if sen[-1] not in {'.', '?', '!', '"'}: continue # Prevent a lot of short sentences
        final_data.append(sen)
    vocab_was_none = vocab is None
    if vocab is None:
        vocab = set()
    for i in range(len(final_data)):
        final_data[i] = [START] + final_data[i] + [END]
        if vocab_was_none:
            for word in final_data[i]:
                vocab.add(word)
    return final_data, vocab

def getDataset():
    dataset = torchtext.datasets.WikiText2(root='.data', split=('train', 'valid'))
    train_dataset, vocab = preprocess(dataset[0])
    test_dataset, _ = preprocess(dataset[1], vocab)

    return train_dataset, test_dataset

if __name__ == '__main__':
    train_dataset, test_dataset = getDataset()

if __name__ == '__main__':
    for x in random.sample(train_dataset, 10):
        print (x)

"""## The language moedel class

4 tpyes of language models are implemented: a unigram model, a smoothed uigram model, a bigram model, a smoothed bigram model.
"""

import math
import random
from collections import defaultdict

class LanguageModel(object):
    def __init__(self, trainCorpus):
        '''
        Initialize and train the model (i.e. estimate the model's underlying probability
        distribution from the training corpus.)
        '''

        return

    def generateSentence(self):
        '''
        Generate a sentence by drawing words according to the model's probability distribution.
        Note: Think about how to set the length of the sentence in a principled way.
        '''

        raise NotImplementedError("Implement generateSentence in each subclass.")

    def getSentenceLogProbability(self, sentence):
        '''
        Calculate the log probability of the sentence provided.
        '''

        raise NotImplementedError("Implement getSentenceProbability in each subclass.")

    def getCorpusPerplexity(self, testCorpus):
        '''
        Calculate the perplexity of the corpus provided.
        '''

        raise NotImplementedError("Implement getCorpusPerplexity in each subclass.")

    def printSentences(self, n):
        '''
        Prints n sentences generated by your model.
        '''

        for i in range(n):
            sent = self.generateSentence()
            prob = self.getSentenceLogProbability(sent)
            print('Log Probability:', prob , '\tSentence:',sent)

"""### Unigram Model"""

from binascii import Error
class UnigramModel(LanguageModel):
    def __init__(self, trainCorpus):
        '''
        Initialize and train the model (i.e. estimate the model's underlying probability
        distribution from the training corpus.)
        'trainCorpus' is a list of sentence where each sentence is a list of words.
        '''
        self.corpus = trainCorpus
        self.model = self.train()


    def train(self):
        '''
        Train the model by calculating the probability of each word in then corpus.
        Return a dictionary which map tokens to their unigram frequency and the total counts of all tokens.
        '''
        model = {}
        total_counts = 0
        for sentence in self.corpus:
            for word in sentence:
                if word != '<s>':
                    total_counts += 1
                    if word in model:
                        model[word] += 1
                    else:
                        model[word] = 1

        for word in model:
            model[word] /= total_counts

        return model


    def generateSentence(self):
        '''
        Generate a sentence using the unigram model, the sentence should be starting with the '<s>' and end with the token '</s>'.
        Return a list of tokens representing the sentence.
        '''
        import random
        # Initialize the sentence
        sentence = []
        sentence.append('<s>')
        # Random select tokens from the dictionary based on the count (which is propotional to the frequency)
        while True:
            word = random.choices(list(self.model.keys()), weights = self.model.values())[0]
            sentence.append(word)
            if word == '</s>':
                break

        return sentence


    def getSentenceLogProbability(self, sentence):
        '''
        Calculate the log probability of the given sentence.
        Input is a list of tokens representing a sentence beginning with '<s>' and ends with '</s>'.
        Output is a float number representing the log probabity of the sentence from the unigram model.
        '''
        import math
        sum_logprob = 0
        for word in sentence[1:]:
            if word not in self.model:
                raise ValueError("Sentence contains tokens that doesn't belong to the corpus")
            sum_logprob += math.log(self.model[word])

        return sum_logprob

    def getCorpusPerplexity(self, testCorpus):
        '''
        Calculate the perplexity of the corpus provided. The begnning token '<s>' is not counted in this case
        The input is the corpus, which is list of sentences
        The output is the perplexity of the corpus
        '''
        import math
        total_log_prob = 0
        total_word_count = 0
        for sentence in testCorpus:
            for word in sentence[1:]:
                total_log_prob += math.log(self.model[word])
                total_word_count += 1
        return math.exp(-total_log_prob / total_word_count)

"""Here's sanity check on the function"""

def sanityCheck(model_type):
    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}

    #	Read in the test corpus
    train_corpus = ["By the Late Classic , a network of few <unk> ( few <unk> ) linked various parts of the city , running for several kilometres through its urban core .",
    "Few people realize how difficult it was to create Sonic 's graphics engine , which allowed for the incredible rate of speed the game 's known for ."]
    test_corpus = ["Classic few parts of the game allowed for few <unk> <unk> incredible city .",
                   "Few <unk> realize the difficult network , which linked the game to Sonic ."]
    train_corpus, _ = preprocess(train_corpus)
    test_corpus, _ = preprocess(test_corpus)
    sentence = preprocess(["Sonic was difficult ."])[0][0]

    # Correct answers
    if model_type == "unigram":
       senprobs = [-19.08542845, -114.5001481799, -108.7963657053, -53.6727664115, -55.4645258807]
       trainPerp, testPerp = 41.3308239726, 38.0122981569
       model = UnigramModel(train_corpus)
    elif model_type == "smoothed-unigram":
       senprobs = [-19.0405293515, -115.3479413049, -108.9114348746, -54.8190029616, -55.8122547346]
       trainPerp, testPerp = 41.9994393615, 39.9531928383
       model = SmoothedUnigramModel(train_corpus)
    elif model_type == "bigram":
       senprobs = [-float('inf'), -10.3450917073, -9.2464794186, -float('inf'), -float('inf')]
       trainPerp, testPerp = 1.3861445461, float('inf')
       model = BigramModel(train_corpus)
    elif model_type == "smoothed-bigram":
       senprobs = [-16.355820202, -76.0026113319, -74.2346475108, -47.2885760372, -51.2730261907]
       trainPerp, testPerp = 12.2307627397, 26.7193157699
       model = SmoothedBigramModelAD(train_corpus)
    else: assert False, 'Invalid model_type'

    print("--- TEST: generateSentence() ---")
    modelSen = model.generateSentence()
    senTestPassed = isinstance(modelSen, list) and len(modelSen) > 1 and isinstance(modelSen[0], str)
    if senTestPassed:
        print ("Test generateSentence() passed!")
    else:
        print ("Test generateSentence() failed; did not return a list of strings...")

    print("\n--- TEST: getSentenceLogProbability(...) ---")
    sentences = [sentence, *train_corpus, *test_corpus]
    failed = 0
    for i in range(len(sentences)):
        sen, correct_prob = sentences[i], senprobs[i]
        prob = round(model.getSentenceLogProbability(sen), 10)
        print("Correct log prob.:", correct_prob, '\tYour log prob.:', prob, '\t', 'PASSED' if prob == correct_prob else 'FAILED', '\t', sen)
        if prob != correct_prob: failed+=1

    if not failed:
        print ("Test getSentenceProbability(...) passed!")
    else:
        print("Test getSentenceProbability(...) failed on", failed, "sentence" if failed == 1 else 'sentences...')

    print("\n--- TEST: getCorpusPerplexity(...) ---")
    train_perp = round(model.getCorpusPerplexity(train_corpus), 10)
    test_perp = round(model.getCorpusPerplexity(test_corpus), 10)

    print("Correct train perp.:", trainPerp, '\tYour train perp.:', train_perp, '\t', 'PASSED' if trainPerp == train_perp else 'FAILED')
    print("Correct test perp.:", testPerp, '\tYour test perp.:', test_perp, '\t', 'PASSED' if testPerp == test_perp else 'FAILED')
    train_passed, test_passed = train_perp == trainPerp, test_perp == testPerp
    if train_passed and test_passed:
        print("Test getCorpusPerplexity(...) passed!")
    else:
        print("Test getCorpusPerplexity(...) failed on", "the training corpus and the testing corpus..." if not train_passed and not test_passed else "the testing corpus..." if not test_passed else "the training corpus...")

if __name__=='__main__':
    sanityCheck('unigram')

"""Sanity check with excuting time"""

import time

def sanityCheckFullDataset(model_type):
    model = UnigramModel(train_dataset)
    idxes = list(range(75,7500, 800))
    small_test_corpus = [test_dataset[idx] for idx in idxes]
    if model_type == 'unigram':
        senprobs = [-80.7782190984, -174.4769654449, -136.455148267, -225.5890741503, -719.0142129846, -236.350443633, -126.0056604204, -47.3424655612, -47.7775372096, -138.8159941929]
        testPerp = 881.0132848704
        model = UnigramModel(train_dataset)
    elif model_type == 'smoothed-unigram':
        senprobs = [-80.8423009715, -174.5131424172, -136.3181234818, -225.357454098, -719.1543898871, -236.6682968913, -126.1965419509, -47.4369338195, -47.7692144935, -138.542462715]
        testPerp = 881.6105352831
        model = SmoothedUnigramModel(train_dataset)
    elif model_type == 'bigram':
        senprobs = [-float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -32.1502020637, -float('inf'), -float('inf')]
        testPerp = float ('inf')
        model = BigramModel(train_dataset)
    elif model_type == 'smoothed-bigram':
        senprobs = [-61.3754065648, -141.9754903887, -107.0849366076, -168.4944718788, -619.9409055374, -195.8159911677, -86.3762008156, -32.4764801981, -48.124714509, -124.687107856]
        testPerp = 261.4247123506
        model = SmoothedBigramModelAD(train_dataset)
    else: assert False, 'Invalid model_type'
    print("\n--- TEST: getSentenceLogProbability(...) ---")
    failed = 0
    for i in range(len(small_test_corpus)):
        sen, correct_prob = small_test_corpus[i], senprobs[i]
        prob = round(model.getSentenceLogProbability(sen), 10)
        print("Correct log prob.:", correct_prob, '\tYour log prob.:', prob, '\t', 'PASSED' if prob == correct_prob else 'FAILED', '\t', sen)
        if prob != correct_prob: failed+=1

    if not failed:
        print ("Test getSentenceProbability(...) passed!")
    else:
        print("Test getSentenceProbability(...) failed on", failed, "sentence" if failed == 1 else 'sentences...')

    print("\n--- TEST: getCorpusPerplexity(...) ---")
    test_perp = round(model.getCorpusPerplexity(small_test_corpus), 10)

    print("Correct test perp.:", testPerp, '\tYour test perp.:', test_perp, '\t', 'PASSED' if testPerp == test_perp else 'FAILED')
    test_passed = test_perp == testPerp
    if test_passed:
        print("Test getCorpusPerplexity(...) passed!")
    else:
        print("Test getCorpusPerplexity(...) failed on the testing corpus...")

if __name__=='__main__':
    start_time = time.time()
    sanityCheckFullDataset('unigram')
    end_time = time.time()
    print(f'Execution time: {end_time - start_time} seconds')

"""Train model on the wikitext corpus"""

def runModel(model_type):
    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}
    # Read the corpora
    if model_type == 'unigram':
        model = UnigramModel(train_dataset)
    elif model_type == 'bigram':
        model = BigramModel(train_dataset)
    elif model_type == 'smoothed-unigram':
        model = SmoothedUnigramModel(train_dataset)
    else:
        model = SmoothedBigramModelAD(train_dataset)

    print("--------- 5 sentences from your model ---------")
    model.printSentences(5)

    print ("\n--------- Corpus Perplexities ---------")
    print ("Training Set:", model.getCorpusPerplexity(train_dataset))
    print ("Testing Set:", model.getCorpusPerplexity(test_dataset))

if __name__=='__main__':
    runModel('unigram')

"""### Smoothed Unigram Model"""

class SmoothedUnigramModel(UnigramModel):
    def train(self):
        '''
        Train the model by calculating the probability of each word in then corpus, with the smoothed method
        Return a dictionary which map tokens to their unigram frequency and the total counts of all tokens.
        '''
        model = {}
        total_counts = 0
        for sentence in self.corpus:
            for word in sentence:
                if word != '<s>':
                    total_counts += 1
                    if word in model:
                        model[word] += 1
                    else:
                        model[word] = 1

        number_tokens = len(model.keys())

        for word in model:
            model[word] = (model[word] + 1) / (total_counts + number_tokens)

        return model

if __name__=='__main__':
    sanityCheck('smoothed-unigram')

if __name__=='__main__':
    sanityCheckFullDataset('smoothed-unigram')

if __name__=='__main__':
    runModel('smoothed-unigram')

"""### Bigram Model"""

class BigramModel(LanguageModel):
    def __init__(self, trainCorpus):
        '''
        Initialize and train the model
        Input 'trainCorpus' is a list of sentence where each sentence is a list of words.
        '''
        self.corpus = trainCorpus
        self.model = self.train()

    def train(self):
        '''
        Train the model by calculating the conditional probability of each word in then corpus.
        Return a dictionary of dictionaries stroing the conditional frequency of each word on the previous word
        '''
        model = {}
        for sentence in self.corpus:
            for i in range(len(sentence) - 1):
                word = sentence[i]
                word_next = sentence[i + 1]
                if word in model:
                    if word_next in model[word]:
                        model[word][word_next] += 1
                    else:
                        model[word][word_next] = 1
                else:
                    model[word] = {word_next : 1}

        # Calculate the conditional porbabilities
        for word in model:
            total_next_words = sum(model[word].values())
            for word_next in model[word]:
                model[word][word_next] /= total_next_words

        return model

    def generateSentence(self):
        '''
        Generate sentences based on the bigram model, the sentence starts with the token '<s>' and ends with the token '</s>'
        The output sentence is a list of tokens from the model beginning with '<s>' and ends with '</s>'
        '''
        import random

        # Initialize the sentence
        sentence = []
        sentence.append('<s>')

        curr_word = sentence[0] # Set the current word to be the '<s>'
        while True:
            next_word = random.choices(list(self.model[curr_word].keys()), weights=self.model[curr_word].values())[0]
            sentence.append(next_word)
            curr_word = next_word
            if curr_word == '</s>':
                break # Break if the token generated is '</s>'

        return sentence

    def getSentenceLogProbability(self, sentence):
        '''
        Calculate the conditonal log probability of the sentence
        Input is the list of tokens
        Output is the float number that is the log probability of the sentence
        '''
        import math
        log_prob_sum = 0
        for i in range(len(sentence) - 1):
            curr_word = sentence[i]
            next_word = sentence[i+1]
            if curr_word in self.model and next_word in self.model[curr_word]:
                log_prob_sum += math.log(self.model[curr_word][next_word])
            else:
                log_prob_sum = -math.inf # If there's no such conditional distribution in the model, return -infinity
                break

        return log_prob_sum

    def getCorpusPerplexity(self, testCorpus):
        '''
        Calculate the perplexcity of the testcorpus based on the model from the current corpus
        Input testCorpus is the list of sentences
        Output is a float number representing the perplexity of the input corpus
        '''
        import math
        log_prob_sum = 0
        word_count = 0
        for sentence in testCorpus:
            log_prob_sum += self.getSentenceLogProbability(sentence)
            word_count += len(sentence) - 1 # Don't take '<s>' as the word count

        return math.exp(-log_prob_sum / word_count)

if __name__=='__main__':
    sanityCheck('bigram')

if __name__=='__main__':
    sanityCheckFullDataset('bigram')

if __name__=='__main__':
    runModel('bigram')

"""### Smoothed Bigram Model"""

class SmoothedBigramModelAD(BigramModel):
    def __init__(self, trainCorpus):
        '''
        Initialize and train the model
        Input 'trainCorpus' is a list of sentence where each sentence is a list of words.
        '''
        self.corpus = trainCorpus
        self.model, self.model_count, self.D, self.S = self.train()
        self.unigram = self.get_unirgam()

    def train(self):
        '''
        Train the model with the smoothed method
        Output is a dictionary of dictionaries that represents the smoothed distribution of the model
        '''
        import math
        unigram_model = self.get_unirgam()

        model = {}
        model_count = {}
        for sentence in self.corpus:
            for i in range(len(sentence) - 1):
                word = sentence[i]
                word_next = sentence[i + 1]
                if word in model:
                    if word_next in model[word]:
                        model[word][word_next] += 1
                    else:
                        model[word][word_next] = 1
                else:
                    model[word] = {word_next : 1}

        D, S = self.calculate_DS(model)

        # Calculate the conditional porbabilities
        for word in model:
            total_next_words = sum(model[word].values())
            model_count[word] = total_next_words
            for word_next in model[word]:
                model[word][word_next] = max(model[word][word_next] - D, 0) / total_next_words + (D / total_next_words * S[word] * unigram_model[word_next])

        return model, model_count, D, S

    def calculate_DS(self, model):
        '''
        Calculate the D and S of the model
        '''
        n1 = 0
        n2 = 0
        S = {}
        for word in model:
            S[word] = len(model[word])
            for next_word in model[word]:
                if model[word][next_word] == 1:
                    n1 += 1
                elif model[word][next_word] == 2:
                    n2 += 1

        D = n1 / (n1 + 2 * n2)

        return D, S

    def get_unirgam(self):
        '''
        Get the smoothed unigram model for the corpus
        '''
        model = {}
        total_counts = 0
        for sentence in self.corpus:
            for word in sentence:
                if word != '<s>':
                    total_counts += 1
                    if word in model:
                        model[word] += 1
                    else:
                        model[word] = 1

        number_tokens = len(model.keys())

        for word in model:
            model[word] = (model[word] + 1) / (total_counts + number_tokens)

        return model

    def getSentenceLogProbability(self, sentence):
        '''
        Calculate the conditonal log probability of the sentence
        Input is the list of tokens
        Output is the float number that is the log probability of the sentence
        '''
        import math
        D = self.D
        S = self.S
        unigram = self.unigram
        log_prob_sum = 0
        for i in range(len(sentence) - 1):
            curr_word = sentence[i]
            next_word = sentence[i+1]
            total_next_words = self.model_count[curr_word]
            if curr_word in self.model and next_word in self.model[curr_word]:
                log_prob_sum += math.log(self.model[curr_word][next_word])
            else:
                log_prob_sum += math.log(D / total_next_words * S[curr_word] * unigram[next_word])


        return log_prob_sum

if __name__=='__main__':
    sanityCheck('smoothed-bigram')

if __name__=='__main__':
    sanityCheckFullDataset('smoothed-bigram')

if __name__=='__main__':
    runModel('smoothed-bigram')