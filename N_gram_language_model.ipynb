{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This file builds up several traditional language models on corpus of Wikipedia articles.\n",
        "\n",
        "WikiText-2, a corpus of high-quality Wikipedia articles. The dataset was originally introduced in the following paper: https://arxiv.org/pdf/1609.07843v1.pdf. A raw version of the data can easily be viewed here: https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2.\n",
        "\n",
        "The code was primarily designed to run on Google Colab."
      ],
      "metadata": {
        "id": "OKTvedBiqWLS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAa-88fvpPrf",
        "outputId": "319da9a0-680d-47d5-e7b7-af83f1b97ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchdata==0.5.1\n",
            "  Downloading torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.31.0)\n",
            "Collecting portalocker>=2.0.0 (from torchdata==0.5.1)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting torch==1.13.1 (from torchdata==0.5.1)\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1->torchdata==0.5.1) (4.5.0)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==1.13.1->torchdata==0.5.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==1.13.1->torchdata==0.5.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==1.13.1->torchdata==0.5.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==1.13.1->torchdata==0.5.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata==0.5.1) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchdata==0.5.1) (0.42.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata==0.5.1) (2023.11.17)\n",
            "Installing collected packages: portalocker, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch, torchdata\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.7.0\n",
            "    Uninstalling torchdata-0.7.0:\n",
            "      Successfully uninstalled torchdata-0.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.16.0 requires torchdata==0.7.0, but you have torchdata 0.5.1 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 portalocker-2.8.2 torch-1.13.1 torchdata-0.5.1\n",
            "Collecting torchtext==0.14.0\n",
            "  Downloading torchtext-0.14.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (2.31.0)\n",
            "Collecting torch==1.13.0 (from torchtext==0.14.0)\n",
            "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.0) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (4.5.0)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0->torchtext==0.14.0) (11.7.99)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext==0.14.0) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext==0.14.0) (0.42.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.14.0) (2023.11.17)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1\n",
            "    Uninstalling torch-1.13.1:\n",
            "      Successfully uninstalled torch-1.13.1\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.16.0\n",
            "    Uninstalling torchtext-0.16.0:\n",
            "      Successfully uninstalled torchtext-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\n",
            "torchdata 0.5.1 requires torch==1.13.1, but you have torch 1.13.0 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.0 torchtext-0.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdata==0.5.1\n",
        "!pip install torchtext==0.14.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Data\n",
        "\n",
        "* <b>Sentence splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp; We identify individual sentences by splitting paragraphs at punctuation tokens (\".\",  \"!\",  \"?\").\n",
        "\n",
        "* <b>Sentence markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each sentence must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`).\n",
        "\n",
        "* <b>Unknown words:</b>&nbsp;&nbsp;&nbsp;&nbsp;The WikiText dataset has replaced all works that are not in the corpora as (`<UNK>`)"
      ],
      "metadata": {
        "id": "gRvo9nXpqVqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START = \"<s>\"   # Start-of-sentence token\n",
        "END = \"</s>\"    # End-of-sentence-token\n",
        "UNK = \"<UNK>\"   # Unknown word token"
      ],
      "metadata": {
        "id": "1kN0R-sZrSie"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "import random\n",
        "import sys\n",
        "\n",
        "def preprocess(data, vocab=None):\n",
        "    final_data = []\n",
        "    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "    for paragraph in data:\n",
        "        paragraph = [x if x != '<unk>' else UNK for x in paragraph.split()]\n",
        "        if vocab is not None:\n",
        "            paragraph = [x if x in vocab else UNK for x in paragraph]\n",
        "        if paragraph == [] or paragraph.count('=') >= 2: continue\n",
        "        sen = []\n",
        "        prev_punct, prev_quot = False, False\n",
        "        for word in paragraph:\n",
        "            if prev_quot:\n",
        "                if word[0] not in lowercase:\n",
        "                    final_data.append(sen)\n",
        "                    sen = []\n",
        "                    prev_punct, prev_quot = False, False\n",
        "            if prev_punct:\n",
        "                if word == '\"':\n",
        "                    prev_punct, prev_quot = False, True\n",
        "                else:\n",
        "                    if word[0] not in lowercase:\n",
        "                        final_data.append(sen)\n",
        "                        sen = []\n",
        "                        prev_punct, prev_quot = False, False\n",
        "            if word in {'.', '?', '!'}: prev_punct = True\n",
        "            sen += [word]\n",
        "        if sen[-1] not in {'.', '?', '!', '\"'}: continue # Prevent a lot of short sentences\n",
        "        final_data.append(sen)\n",
        "    vocab_was_none = vocab is None\n",
        "    if vocab is None:\n",
        "        vocab = set()\n",
        "    for i in range(len(final_data)):\n",
        "        final_data[i] = [START] + final_data[i] + [END]\n",
        "        if vocab_was_none:\n",
        "            for word in final_data[i]:\n",
        "                vocab.add(word)\n",
        "    return final_data, vocab\n",
        "\n",
        "def getDataset():\n",
        "    dataset = torchtext.datasets.WikiText2(root='.data', split=('train', 'valid'))\n",
        "    train_dataset, vocab = preprocess(dataset[0])\n",
        "    test_dataset, _ = preprocess(dataset[1], vocab)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_dataset, test_dataset = getDataset()"
      ],
      "metadata": {
        "id": "KFlP0LVmsJN3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    for x in random.sample(train_dataset, 10):\n",
        "        print (x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V73cWAP_sNAK",
        "outputId": "f425cb1a-80db-48fb-8929-ec59b95d8a45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'His', 'wife', 'Connie', ',', 'whom', 'he', 'had', 'known', 'since', 'childhood', ',', 'ran', 'the', 'business', 'in', 'his', 'absence', '.', '</s>']\n",
            "['<s>', '<UNK>', '<UNK>', ',', 'a', 'member', 'of', 'Anonymous', ',', 'commented', 'to', '<UNK>', 'about', 'the', 'Church', 'of', 'Scientology', \"'s\", 'denial', 'of', 'its', '\"', 'Fair', 'Game', '\"', 'policy', ':', '\"', 'Even', 'if', 'the', 'name', \"'\", 'fair', 'game', \"'\", 'is', 'not', 'in', 'use', ',', 'the', 'Church', 'of', 'Scientology', 'is', 'an', 'organisation', 'that', 'continues', 'to', 'practice', 'a', 'vicious', 'policy', 'of', '<UNK>', 'against', 'perceived', 'enemies', ',', 'and', 'it', 'teaches', 'its', 'members', 'that', 'extreme', 'measures', 'are', 'morally', 'justified', 'if', 'they', 'aid', 'the', 'Church', '.', '\"', '</s>']\n",
            "['<s>', 'In', 'general', ',', 'this', 'ordering', 'is', 'not', 'unique', ';', 'a', 'DAG', 'has', 'a', 'unique', 'topological', 'ordering', 'if', 'and', 'only', 'if', 'it', 'has', 'a', 'directed', 'path', 'containing', 'all', 'the', 'vertices', ',', 'in', 'which', 'case', 'the', 'ordering', 'is', 'the', 'same', 'as', 'the', 'order', 'in', 'which', 'the', 'vertices', 'appear', 'in', 'the', 'path', '.', '</s>']\n",
            "['<s>', 'About', '4', '%', 'of', 'the', 'Republic', \"'s\", 'population', 'and', 'about', '14', '%', 'of', 'the', 'Northern', 'Ireland', 'population', 'describe', 'themselves', 'as', 'of', 'no', 'religion', '.', '</s>']\n",
            "['<s>', 'It', 'was', 'used', 'as', 'a', 'staging', 'ground', 'for', 'a', 'number', 'of', 'attacks', 'on', 'North', 'Vietnamese', '(', '<UNK>', ')', 'troop', 'movements', 'down', 'the', 'Ho', 'Chi', 'Minh', 'Trail', '.', '</s>']\n",
            "['<s>', 'In', '2009', ',', 'the', 'Michigan', 'Department', 'of', 'Natural', 'Resources', 'confirmed', 'a', 'cougar', 'sighting', 'in', 'Michigan', \"'s\", 'Upper', 'Peninsula', '.', '</s>']\n",
            "['<s>', 'It', 'acquired', 'a', 'Nielsen', 'rating', 'of', '13', '@.@', '0', ',', 'and', 'was', 'the', 'highest', '@-@', 'rated', 'show', 'on', 'the', 'Fox', 'network', 'the', 'week', 'it', 'aired', '.', '</s>']\n",
            "['<s>', 'Mazza', 'produced', 'the', 'first', 'scientific', 'confirmation', 'of', 'the', 'existence', 'of', 'Trypanosoma', 'cruzi', 'in', 'Argentina', 'in', '1927', ',', 'eventually', 'leading', 'to', 'support', 'from', 'local', 'and', 'European', 'medical', 'schools', 'and', 'Argentine', 'government', 'policy', 'makers', '.', '</s>']\n",
            "['<s>', 'M', '@-@', '6', ',', 'or', 'the', 'Paul', 'B.', 'Henry', 'Freeway', ',', 'is', 'a', '19', '@.@', '696', '@-@', 'mile', '(', '31', '@.@', '<UNK>', 'km', ')', 'freeway', 'and', 'state', 'trunkline', 'highway', 'in', 'the', 'United', 'States', 'that', 'serves', 'portions', 'of', 'southern', 'Kent', 'and', 'eastern', 'Ottawa', 'counties', 'south', 'of', 'Grand', 'Rapids', ',', 'Michigan', '.', '</s>']\n",
            "['<s>', 'It', 'is', 'a', 'historical', 'saga', 'of', 'Malaya', 'which', 'traces', 'the', 'years', '1922', 'to', '1982', ',', 'telling', 'the', 'story', 'of', 'Ya', '<UNK>', ',', 'from', 'his', 'family', \"'s\", 'migration', 'from', 'South', 'Thailand', 'to', '<UNK>', 'after', 'his', 'father', \"'s\", 'death', '.', '</s>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The language moedel class\n",
        "\n",
        "4 tpyes of language models are implemented: a unigram model, a smoothed uigram model, a bigram model, a smoothed bigram model."
      ],
      "metadata": {
        "id": "I9WO8oA8srdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class LanguageModel(object):\n",
        "    def __init__(self, trainCorpus):\n",
        "        '''\n",
        "        Initialize and train the model (i.e. estimate the model's underlying probability\n",
        "        distribution from the training corpus.)\n",
        "        '''\n",
        "\n",
        "        return\n",
        "\n",
        "    def generateSentence(self):\n",
        "        '''\n",
        "        Generate a sentence by drawing words according to the model's probability distribution.\n",
        "        Note: Think about how to set the length of the sentence in a principled way.\n",
        "        '''\n",
        "\n",
        "        raise NotImplementedError(\"Implement generateSentence in each subclass.\")\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        '''\n",
        "        Calculate the log probability of the sentence provided.\n",
        "        '''\n",
        "\n",
        "        raise NotImplementedError(\"Implement getSentenceProbability in each subclass.\")\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        '''\n",
        "        Calculate the perplexity of the corpus provided.\n",
        "        '''\n",
        "\n",
        "        raise NotImplementedError(\"Implement getCorpusPerplexity in each subclass.\")\n",
        "\n",
        "    def printSentences(self, n):\n",
        "        '''\n",
        "        Prints n sentences generated by your model.\n",
        "        '''\n",
        "\n",
        "        for i in range(n):\n",
        "            sent = self.generateSentence()\n",
        "            prob = self.getSentenceLogProbability(sent)\n",
        "            print('Log Probability:', prob , '\\tSentence:',sent)"
      ],
      "metadata": {
        "id": "MmrFhTodtgIA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unigram Model"
      ],
      "metadata": {
        "id": "on2qCWbJtquz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from binascii import Error\n",
        "class UnigramModel(LanguageModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "        '''\n",
        "        Initialize and train the model (i.e. estimate the model's underlying probability\n",
        "        distribution from the training corpus.)\n",
        "        'trainCorpus' is a list of sentence where each sentence is a list of words.\n",
        "        '''\n",
        "        self.corpus = trainCorpus\n",
        "        self.model = self.train()\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        '''\n",
        "        Train the model by calculating the probability of each word in then corpus.\n",
        "        Return a dictionary which map tokens to their unigram frequency and the total counts of all tokens.\n",
        "        '''\n",
        "        model = {}\n",
        "        total_counts = 0\n",
        "        for sentence in self.corpus:\n",
        "            for word in sentence:\n",
        "                if word != '<s>':\n",
        "                    total_counts += 1\n",
        "                    if word in model:\n",
        "                        model[word] += 1\n",
        "                    else:\n",
        "                        model[word] = 1\n",
        "\n",
        "        for word in model:\n",
        "            model[word] /= total_counts\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    def generateSentence(self):\n",
        "        '''\n",
        "        Generate a sentence using the unigram model, the sentence should be starting with the '<s>' and end with the token '</s>'.\n",
        "        Return a list of tokens representing the sentence.\n",
        "        '''\n",
        "        import random\n",
        "        # Initialize the sentence\n",
        "        sentence = []\n",
        "        sentence.append('<s>')\n",
        "        # Random select tokens from the dictionary based on the count (which is propotional to the frequency)\n",
        "        while True:\n",
        "            word = random.choices(list(self.model.keys()), weights = self.model.values())[0]\n",
        "            sentence.append(word)\n",
        "            if word == '</s>':\n",
        "                break\n",
        "\n",
        "        return sentence\n",
        "\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        '''\n",
        "        Calculate the log probability of the given sentence.\n",
        "        Input is a list of tokens representing a sentence beginning with '<s>' and ends with '</s>'.\n",
        "        Output is a float number representing the log probabity of the sentence from the unigram model.\n",
        "        '''\n",
        "        import math\n",
        "        sum_logprob = 0\n",
        "        for word in sentence[1:]:\n",
        "            if word not in self.model:\n",
        "                raise ValueError(\"Sentence contains tokens that doesn't belong to the corpus\")\n",
        "            sum_logprob += math.log(self.model[word])\n",
        "\n",
        "        return sum_logprob\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        '''\n",
        "        Calculate the perplexity of the corpus provided. The begnning token '<s>' is not counted in this case\n",
        "        The input is the corpus, which is list of sentences\n",
        "        The output is the perplexity of the corpus\n",
        "        '''\n",
        "        import math\n",
        "        total_log_prob = 0\n",
        "        total_word_count = 0\n",
        "        for sentence in testCorpus:\n",
        "            for word in sentence[1:]:\n",
        "                total_log_prob += math.log(self.model[word])\n",
        "                total_word_count += 1\n",
        "        return math.exp(-total_log_prob / total_word_count)\n"
      ],
      "metadata": {
        "id": "-DEFpgD_tnUH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's sanity check on the function"
      ],
      "metadata": {
        "id": "uVeeG6vItzmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sanityCheck(model_type):\n",
        "    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n",
        "\n",
        "    #\tRead in the test corpus\n",
        "    train_corpus = [\"By the Late Classic , a network of few <unk> ( few <unk> ) linked various parts of the city , running for several kilometres through its urban core .\",\n",
        "    \"Few people realize how difficult it was to create Sonic 's graphics engine , which allowed for the incredible rate of speed the game 's known for .\"]\n",
        "    test_corpus = [\"Classic few parts of the game allowed for few <unk> <unk> incredible city .\",\n",
        "                   \"Few <unk> realize the difficult network , which linked the game to Sonic .\"]\n",
        "    train_corpus, _ = preprocess(train_corpus)\n",
        "    test_corpus, _ = preprocess(test_corpus)\n",
        "    sentence = preprocess([\"Sonic was difficult .\"])[0][0]\n",
        "\n",
        "    # Correct answers\n",
        "    if model_type == \"unigram\":\n",
        "       senprobs = [-19.08542845, -114.5001481799, -108.7963657053, -53.6727664115, -55.4645258807]\n",
        "       trainPerp, testPerp = 41.3308239726, 38.0122981569\n",
        "       model = UnigramModel(train_corpus)\n",
        "    elif model_type == \"smoothed-unigram\":\n",
        "       senprobs = [-19.0405293515, -115.3479413049, -108.9114348746, -54.8190029616, -55.8122547346]\n",
        "       trainPerp, testPerp = 41.9994393615, 39.9531928383\n",
        "       model = SmoothedUnigramModel(train_corpus)\n",
        "    elif model_type == \"bigram\":\n",
        "       senprobs = [-float('inf'), -10.3450917073, -9.2464794186, -float('inf'), -float('inf')]\n",
        "       trainPerp, testPerp = 1.3861445461, float('inf')\n",
        "       model = BigramModel(train_corpus)\n",
        "    elif model_type == \"smoothed-bigram\":\n",
        "       senprobs = [-16.355820202, -76.0026113319, -74.2346475108, -47.2885760372, -51.2730261907]\n",
        "       trainPerp, testPerp = 12.2307627397, 26.7193157699\n",
        "       model = SmoothedBigramModelAD(train_corpus)\n",
        "    else: assert False, 'Invalid model_type'\n",
        "\n",
        "    print(\"--- TEST: generateSentence() ---\")\n",
        "    modelSen = model.generateSentence()\n",
        "    senTestPassed = isinstance(modelSen, list) and len(modelSen) > 1 and isinstance(modelSen[0], str)\n",
        "    if senTestPassed:\n",
        "        print (\"Test generateSentence() passed!\")\n",
        "    else:\n",
        "        print (\"Test generateSentence() failed; did not return a list of strings...\")\n",
        "\n",
        "    print(\"\\n--- TEST: getSentenceLogProbability(...) ---\")\n",
        "    sentences = [sentence, *train_corpus, *test_corpus]\n",
        "    failed = 0\n",
        "    for i in range(len(sentences)):\n",
        "        sen, correct_prob = sentences[i], senprobs[i]\n",
        "        prob = round(model.getSentenceLogProbability(sen), 10)\n",
        "        print(\"Correct log prob.:\", correct_prob, '\\tYour log prob.:', prob, '\\t', 'PASSED' if prob == correct_prob else 'FAILED', '\\t', sen)\n",
        "        if prob != correct_prob: failed+=1\n",
        "\n",
        "    if not failed:\n",
        "        print (\"Test getSentenceProbability(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getSentenceProbability(...) failed on\", failed, \"sentence\" if failed == 1 else 'sentences...')\n",
        "\n",
        "    print(\"\\n--- TEST: getCorpusPerplexity(...) ---\")\n",
        "    train_perp = round(model.getCorpusPerplexity(train_corpus), 10)\n",
        "    test_perp = round(model.getCorpusPerplexity(test_corpus), 10)\n",
        "\n",
        "    print(\"Correct train perp.:\", trainPerp, '\\tYour train perp.:', train_perp, '\\t', 'PASSED' if trainPerp == train_perp else 'FAILED')\n",
        "    print(\"Correct test perp.:\", testPerp, '\\tYour test perp.:', test_perp, '\\t', 'PASSED' if testPerp == test_perp else 'FAILED')\n",
        "    train_passed, test_passed = train_perp == trainPerp, test_perp == testPerp\n",
        "    if train_passed and test_passed:\n",
        "        print(\"Test getCorpusPerplexity(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getCorpusPerplexity(...) failed on\", \"the training corpus and the testing corpus...\" if not train_passed and not test_passed else \"the testing corpus...\" if not test_passed else \"the training corpus...\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    sanityCheck('unigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJRiT4zwt4kG",
        "outputId": "7b2ad8e6-b655-418f-df73-55bf3b2b5d01"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -19.08542845 \tYour log prob.: -19.08542845 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -114.5001481799 \tYour log prob.: -114.5001481799 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -108.7963657053 \tYour log prob.: -108.7963657053 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -53.6727664115 \tYour log prob.: -53.6727664115 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -55.4645258807 \tYour log prob.: -55.4645258807 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 41.3308239726 \tYour train perp.: 41.3308239726 \t PASSED\n",
            "Correct test perp.: 38.0122981569 \tYour test perp.: 38.0122981569 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity check with excuting time"
      ],
      "metadata": {
        "id": "ZWAaeABTuJdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def sanityCheckFullDataset(model_type):\n",
        "    model = UnigramModel(train_dataset)\n",
        "    idxes = list(range(75,7500, 800))\n",
        "    small_test_corpus = [test_dataset[idx] for idx in idxes]\n",
        "    if model_type == 'unigram':\n",
        "        senprobs = [-80.7782190984, -174.4769654449, -136.455148267, -225.5890741503, -719.0142129846, -236.350443633, -126.0056604204, -47.3424655612, -47.7775372096, -138.8159941929]\n",
        "        testPerp = 881.0132848704\n",
        "        model = UnigramModel(train_dataset)\n",
        "    elif model_type == 'smoothed-unigram':\n",
        "        senprobs = [-80.8423009715, -174.5131424172, -136.3181234818, -225.357454098, -719.1543898871, -236.6682968913, -126.1965419509, -47.4369338195, -47.7692144935, -138.542462715]\n",
        "        testPerp = 881.6105352831\n",
        "        model = SmoothedUnigramModel(train_dataset)\n",
        "    elif model_type == 'bigram':\n",
        "        senprobs = [-float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -float('inf'), -32.1502020637, -float('inf'), -float('inf')]\n",
        "        testPerp = float ('inf')\n",
        "        model = BigramModel(train_dataset)\n",
        "    elif model_type == 'smoothed-bigram':\n",
        "        senprobs = [-61.3754065648, -141.9754903887, -107.0849366076, -168.4944718788, -619.9409055374, -195.8159911677, -86.3762008156, -32.4764801981, -48.124714509, -124.687107856]\n",
        "        testPerp = 261.4247123506\n",
        "        model = SmoothedBigramModelAD(train_dataset)\n",
        "    else: assert False, 'Invalid model_type'\n",
        "    print(\"\\n--- TEST: getSentenceLogProbability(...) ---\")\n",
        "    failed = 0\n",
        "    for i in range(len(small_test_corpus)):\n",
        "        sen, correct_prob = small_test_corpus[i], senprobs[i]\n",
        "        prob = round(model.getSentenceLogProbability(sen), 10)\n",
        "        print(\"Correct log prob.:\", correct_prob, '\\tYour log prob.:', prob, '\\t', 'PASSED' if prob == correct_prob else 'FAILED', '\\t', sen)\n",
        "        if prob != correct_prob: failed+=1\n",
        "\n",
        "    if not failed:\n",
        "        print (\"Test getSentenceProbability(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getSentenceProbability(...) failed on\", failed, \"sentence\" if failed == 1 else 'sentences...')\n",
        "\n",
        "    print(\"\\n--- TEST: getCorpusPerplexity(...) ---\")\n",
        "    test_perp = round(model.getCorpusPerplexity(small_test_corpus), 10)\n",
        "\n",
        "    print(\"Correct test perp.:\", testPerp, '\\tYour test perp.:', test_perp, '\\t', 'PASSED' if testPerp == test_perp else 'FAILED')\n",
        "    test_passed = test_perp == testPerp\n",
        "    if test_passed:\n",
        "        print(\"Test getCorpusPerplexity(...) passed!\")\n",
        "    else:\n",
        "        print(\"Test getCorpusPerplexity(...) failed on the testing corpus...\")\n",
        "\n",
        "if __name__=='__main__':\n",
        "    start_time = time.time()\n",
        "    sanityCheckFullDataset('unigram')\n",
        "    end_time = time.time()\n",
        "    print(f'Execution time: {end_time - start_time} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dXGcb1wuNfv",
        "outputId": "f459120d-6ae5-47f2-f886-d1e14f397199"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -80.7782190984 \tYour log prob.: -80.7782190984 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -174.4769654449 \tYour log prob.: -174.4769654449 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -136.455148267 \tYour log prob.: -136.455148267 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -225.5890741503 \tYour log prob.: -225.5890741503 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -719.0142129846 \tYour log prob.: -719.0142129846 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -236.350443633 \tYour log prob.: -236.350443633 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -126.0056604204 \tYour log prob.: -126.0056604204 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -47.3424655612 \tYour log prob.: -47.3424655612 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -47.7775372096 \tYour log prob.: -47.7775372096 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -138.8159941929 \tYour log prob.: -138.8159941929 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: 881.0132848704 \tYour test perp.: 881.0132848704 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n",
            "Execution time: 1.4286949634552002 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model on the wikitext corpus"
      ],
      "metadata": {
        "id": "aL8mtpF_vA-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def runModel(model_type):\n",
        "    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n",
        "    # Read the corpora\n",
        "    if model_type == 'unigram':\n",
        "        model = UnigramModel(train_dataset)\n",
        "    elif model_type == 'bigram':\n",
        "        model = BigramModel(train_dataset)\n",
        "    elif model_type == 'smoothed-unigram':\n",
        "        model = SmoothedUnigramModel(train_dataset)\n",
        "    else:\n",
        "        model = SmoothedBigramModelAD(train_dataset)\n",
        "\n",
        "    print(\"--------- 5 sentences from your model ---------\")\n",
        "    model.printSentences(5)\n",
        "\n",
        "    print (\"\\n--------- Corpus Perplexities ---------\")\n",
        "    print (\"Training Set:\", model.getCorpusPerplexity(train_dataset))\n",
        "    print (\"Testing Set:\", model.getCorpusPerplexity(test_dataset))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    runModel('unigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfTQLzeou9BB",
        "outputId": "f4d29935-71a5-433b-ef16-41b26cb1eb43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -181.94414694213157 \tSentence: ['<s>', 'whose', 'narrates', 'of', 'and', 'explore', 'Gray', ',', 'of', 'was', 'treat', ',', 'Madrid', 'given', 'NME', ')', 'operated', 'the', ',', 'The', 'in', 'westward', '41', 'to', 'time', 'within', 'the', '</s>']\n",
            "Log Probability: -518.4968791802588 \tSentence: ['<s>', 'xenon', 'halfway', '\"', 'than', 'cede', '.', 'about', 'while', '1', '@,@', 'upon', 'and', 'the', 'with', 'majority', 'eleven', ',', 'in', 'parents', 'finally', 'order', 'Carolina', 'Moravec', 'January', 'with', '\"', 'the', '@-@', 'Nathan', 'that', 'a', 'bit', 'him', 'the', '@-@', 'open', 'his', 'moved', 'the', 'placed', 'the', 'Army', 'capable', 'battalions', 'and', '<UNK>', 'from', '1975', 'early', 'acceptance', 'train', ',', 'reach', 'the', 'didn', 'play', 'km', 'feather', 'in', 'of', 'a', 'Second', ',', 'most', 'experience', 'would', 'to', 'York', 'tying', 'includes', 'legal', '@-@', '.', '.', '</s>']\n",
            "Log Probability: -334.593026537898 \tSentence: ['<s>', 'cost', 'many', 'who', 'pool', 'is', 'a', 'and', 'December', 'to', 'As', ',', 'chair', 'to', '<UNK>', 'populations', ',', 'Rachel', 'is', 'gaining', ',', 'al', 'that', '\"', 'and', 'work', 'mate', 'there', '\"', 'respectively', 'to', 'and', 'that', '76ers', 'and', 'competing', 'was', 'fire', 'to', 'an', '.', 'Army', 'with', 'infections', 'captured', 'the', 'intersection', 'to', 'storm', 'population', 'and', '</s>']\n",
            "Log Probability: -140.50495623935356 \tSentence: ['<s>', '114', 'recognized', 'army', ',', 'Pacific', 'of', 'Domoina', 'sales', 'and', 'books', 'as', 'is', 'a', 'terms', \"'s\", 'pay', 'battle', '\"', 'by', '</s>']\n",
            "Log Probability: -9.757369895467326 \tSentence: ['<s>', 'has', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 1101.9435880270637\n",
            "Testing Set: 912.1574385914788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smoothed Unigram Model"
      ],
      "metadata": {
        "id": "mkKUq9m1vSV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SmoothedUnigramModel(UnigramModel):\n",
        "    def train(self):\n",
        "        '''\n",
        "        Train the model by calculating the probability of each word in then corpus, with the smoothed method\n",
        "        Return a dictionary which map tokens to their unigram frequency and the total counts of all tokens.\n",
        "        '''\n",
        "        model = {}\n",
        "        total_counts = 0\n",
        "        for sentence in self.corpus:\n",
        "            for word in sentence:\n",
        "                if word != '<s>':\n",
        "                    total_counts += 1\n",
        "                    if word in model:\n",
        "                        model[word] += 1\n",
        "                    else:\n",
        "                        model[word] = 1\n",
        "\n",
        "        number_tokens = len(model.keys())\n",
        "\n",
        "        for word in model:\n",
        "            model[word] = (model[word] + 1) / (total_counts + number_tokens)\n",
        "\n",
        "        return model"
      ],
      "metadata": {
        "id": "6M0JtlvEvWGS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheck('smoothed-unigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5noV0hBvcZ4",
        "outputId": "a1d8ca3e-d1db-4695-912e-fae2eaf2120a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -19.0405293515 \tYour log prob.: -19.0405293515 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -115.3479413049 \tYour log prob.: -115.3479413049 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -108.9114348746 \tYour log prob.: -108.9114348746 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -54.8190029616 \tYour log prob.: -54.8190029616 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -55.8122547346 \tYour log prob.: -55.8122547346 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 41.9994393615 \tYour train perp.: 41.9994393615 \t PASSED\n",
            "Correct test perp.: 39.9531928383 \tYour test perp.: 39.9531928383 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheckFullDataset('smoothed-unigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaVKM-8-vfJe",
        "outputId": "2be34c3d-d4ca-4dee-b8a6-035707d3dcf6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -80.8423009715 \tYour log prob.: -80.8423009715 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -174.5131424172 \tYour log prob.: -174.5131424172 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -136.3181234818 \tYour log prob.: -136.3181234818 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -225.357454098 \tYour log prob.: -225.357454098 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -719.1543898871 \tYour log prob.: -719.1543898871 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -236.6682968913 \tYour log prob.: -236.6682968913 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -126.1965419509 \tYour log prob.: -126.1965419509 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -47.4369338195 \tYour log prob.: -47.4369338195 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -47.7692144935 \tYour log prob.: -47.7692144935 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -138.542462715 \tYour log prob.: -138.542462715 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: 881.6105352831 \tYour test perp.: 881.6105352831 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    runModel('smoothed-unigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GDADmQxvgko",
        "outputId": "39e5c642-e862-456a-ad89-2be2587c58a8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -142.36721236811863 \tSentence: ['<s>', '\"', 'died', 'Dunster', '.', 'sniping', 'are', 'and', 'New', 'stayed', 'March', 'flotilla', 'corn', 'the', 'two', 'is', 'of', 'was', 'entire', 'the', '<UNK>', '</s>']\n",
            "Log Probability: -281.15924546428084 \tSentence: ['<s>', 'command', '(', ',', 'season', 'the', ',', 'two', 'errors', 'well', 'brought', '5', 'Cruiser', 'Crush', 'cities', 'notes', 'presenter', 'economics', '\"', '@-@', 'rest', 'number', 'Britons', 'started', 'sub', 'a', 'time', 'free', 'an', 'mass', 'Banai', 'series', 'a', 'the', 'instead', 'what', 'from', '</s>']\n",
            "Log Probability: -431.3093453247379 \tSentence: ['<s>', 'for', '.', 'region', 'the', '<UNK>', 'just', 'as', 'Rapids', 'of', '<UNK>', 'before', '00', 'frequently', 'one', '<UNK>', 'particularly', '467', 'and', 'sic', '<UNK>', 'grandfather', 'the', 'period', 'rest', ',', 'Loch', 'research', '.', 'with', '<UNK>', 'then', 'a', 'the', 'of', 'City', 'resignation', ',', 'of', 'less', 'used', 'smuggled', ';', 'keyboards', \"'\", 'each', '<UNK>', '<UNK>', '.', 'also', '14', 'ultimate', ',', 'breeder', 'of', 'for', 'this', 'her', 'German', 'separates', ',', 'an', 'year', 'throughout', 'The', '</s>']\n",
            "Log Probability: -201.74391501081018 \tSentence: ['<s>', 'whose', 'M.', 'name', 'plentiful', 'was', 'and', 'other', 'singing', \"'s\", 'dancer', 'missions', '.', 'Duchess', 'important', '@-@', 'and', '@-@', 'caused', 'kickoff', '.', 'Billboard', 'pistol', 'in', 'was', '@-@', 'Tropical', 'of', '</s>']\n",
            "Log Probability: -132.80344269628714 \tSentence: ['<s>', '2000', 'a', 'flank', 'was', 'the', '–', 'laid', 'to', '.', '\"', 'he', 'he', 'by', 'and', 'was', 'of', 'featuring', '.', 'spreading', 'matches', 'episode', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 1103.0243317913958\n",
            "Testing Set: 914.4724502277719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigram Model"
      ],
      "metadata": {
        "id": "_Y7ikr-Fvs90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramModel(LanguageModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "        '''\n",
        "        Initialize and train the model\n",
        "        Input 'trainCorpus' is a list of sentence where each sentence is a list of words.\n",
        "        '''\n",
        "        self.corpus = trainCorpus\n",
        "        self.model = self.train()\n",
        "\n",
        "    def train(self):\n",
        "        '''\n",
        "        Train the model by calculating the conditional probability of each word in then corpus.\n",
        "        Return a dictionary of dictionaries stroing the conditional frequency of each word on the previous word\n",
        "        '''\n",
        "        model = {}\n",
        "        for sentence in self.corpus:\n",
        "            for i in range(len(sentence) - 1):\n",
        "                word = sentence[i]\n",
        "                word_next = sentence[i + 1]\n",
        "                if word in model:\n",
        "                    if word_next in model[word]:\n",
        "                        model[word][word_next] += 1\n",
        "                    else:\n",
        "                        model[word][word_next] = 1\n",
        "                else:\n",
        "                    model[word] = {word_next : 1}\n",
        "\n",
        "        # Calculate the conditional porbabilities\n",
        "        for word in model:\n",
        "            total_next_words = sum(model[word].values())\n",
        "            for word_next in model[word]:\n",
        "                model[word][word_next] /= total_next_words\n",
        "\n",
        "        return model\n",
        "\n",
        "    def generateSentence(self):\n",
        "        '''\n",
        "        Generate sentences based on the bigram model, the sentence starts with the token '<s>' and ends with the token '</s>'\n",
        "        The output sentence is a list of tokens from the model beginning with '<s>' and ends with '</s>'\n",
        "        '''\n",
        "        import random\n",
        "\n",
        "        # Initialize the sentence\n",
        "        sentence = []\n",
        "        sentence.append('<s>')\n",
        "\n",
        "        curr_word = sentence[0] # Set the current word to be the '<s>'\n",
        "        while True:\n",
        "            next_word = random.choices(list(self.model[curr_word].keys()), weights=self.model[curr_word].values())[0]\n",
        "            sentence.append(next_word)\n",
        "            curr_word = next_word\n",
        "            if curr_word == '</s>':\n",
        "                break # Break if the token generated is '</s>'\n",
        "\n",
        "        return sentence\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        '''\n",
        "        Calculate the conditonal log probability of the sentence\n",
        "        Input is the list of tokens\n",
        "        Output is the float number that is the log probability of the sentence\n",
        "        '''\n",
        "        import math\n",
        "        log_prob_sum = 0\n",
        "        for i in range(len(sentence) - 1):\n",
        "            curr_word = sentence[i]\n",
        "            next_word = sentence[i+1]\n",
        "            if curr_word in self.model and next_word in self.model[curr_word]:\n",
        "                log_prob_sum += math.log(self.model[curr_word][next_word])\n",
        "            else:\n",
        "                log_prob_sum = -math.inf # If there's no such conditional distribution in the model, return -infinity\n",
        "                break\n",
        "\n",
        "        return log_prob_sum\n",
        "\n",
        "    def getCorpusPerplexity(self, testCorpus):\n",
        "        '''\n",
        "        Calculate the perplexcity of the testcorpus based on the model from the current corpus\n",
        "        Input testCorpus is the list of sentences\n",
        "        Output is a float number representing the perplexity of the input corpus\n",
        "        '''\n",
        "        import math\n",
        "        log_prob_sum = 0\n",
        "        word_count = 0\n",
        "        for sentence in testCorpus:\n",
        "            log_prob_sum += self.getSentenceLogProbability(sentence)\n",
        "            word_count += len(sentence) - 1 # Don't take '<s>' as the word count\n",
        "\n",
        "        return math.exp(-log_prob_sum / word_count)\n"
      ],
      "metadata": {
        "id": "IDDbUrWdvwfw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheck('bigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUbZURWQv1Xp",
        "outputId": "93c49af5-431b-44fb-ffac-7c232cbf9fa9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -10.3450917073 \tYour log prob.: -10.3450917073 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -9.2464794186 \tYour log prob.: -9.2464794186 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 1.3861445461 \tYour train perp.: 1.3861445461 \t PASSED\n",
            "Correct test perp.: inf \tYour test perp.: inf \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheckFullDataset('bigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AII9G5HAv3JU",
        "outputId": "c7a0639b-242b-4b08-9829-16e846137e7a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -32.1502020637 \tYour log prob.: -32.1502020637 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -inf \tYour log prob.: -inf \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: inf \tYour test perp.: inf \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    runModel('bigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOUHYB90v4y_",
        "outputId": "2b92f0b1-363a-4107-a758-9af4748f2862"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -81.92438345043252 \tSentence: ['<s>', 'FRELIMO', 'and', '212', '@.@', '1', 'seconds', 'up', 'about', 'the', 'operation', 'within', 'the', 'central', '@-@', 'director', '.', '</s>']\n",
            "Log Probability: -25.713749518017064 \tSentence: ['<s>', 'Between', '1940', 'while', 'strengthening', '.', '</s>']\n",
            "Log Probability: -324.25078362611964 \tSentence: ['<s>', 'The', 'French', ',', 'Fuel', '@-@', 'president', 'of', 'War', 'II', 'of', 'the', 'University', 'of', 'a', '<UNK>', 'and', 'endearingly', 'flawed', ',', 'and', 'the', 'offer', 'believing', 'Robert', 'Dudley', 'seems', 'to', 'the', 'apparent', 'response', ',', 'Jan', '<UNK>', '\"', 'over', '10', ',', 'and', 'comprised', '41', 'feet', '6', 'between', '1984', ',', 'is', 'hired', 'by', 'The', 'Cooper', ',', 'and', 'his', 'life', 'of', 'a', 'large', 'donations', 'were', 'also', 'found', 'to', 'usually', 'removed', 'before', 'the', 'famous', 'classical', 'themes', 'of', 'his', 'immune', 'responses', '.', '</s>']\n",
            "Log Probability: -25.75420705035226 \tSentence: ['<s>', 'Carey', 'is', 'a', 'committee', '.', '</s>']\n",
            "Log Probability: -53.24080820040106 \tSentence: ['<s>', 'Maggie', 'was', 'NC', 'State', 'Attorney', 'series', \"'\", 'rush', 'of', 'month', '.', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 76.92394608735728\n",
            "Testing Set: inf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smoothed Bigram Model"
      ],
      "metadata": {
        "id": "fU8PWllpv7IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SmoothedBigramModelAD(BigramModel):\n",
        "    def __init__(self, trainCorpus):\n",
        "        '''\n",
        "        Initialize and train the model\n",
        "        Input 'trainCorpus' is a list of sentence where each sentence is a list of words.\n",
        "        '''\n",
        "        self.corpus = trainCorpus\n",
        "        self.model, self.model_count, self.D, self.S = self.train()\n",
        "        self.unigram = self.get_unirgam()\n",
        "\n",
        "    def train(self):\n",
        "        '''\n",
        "        Train the model with the smoothed method\n",
        "        Output is a dictionary of dictionaries that represents the smoothed distribution of the model\n",
        "        '''\n",
        "        import math\n",
        "        unigram_model = self.get_unirgam()\n",
        "\n",
        "        model = {}\n",
        "        model_count = {}\n",
        "        for sentence in self.corpus:\n",
        "            for i in range(len(sentence) - 1):\n",
        "                word = sentence[i]\n",
        "                word_next = sentence[i + 1]\n",
        "                if word in model:\n",
        "                    if word_next in model[word]:\n",
        "                        model[word][word_next] += 1\n",
        "                    else:\n",
        "                        model[word][word_next] = 1\n",
        "                else:\n",
        "                    model[word] = {word_next : 1}\n",
        "\n",
        "        D, S = self.calculate_DS(model)\n",
        "\n",
        "        # Calculate the conditional porbabilities\n",
        "        for word in model:\n",
        "            total_next_words = sum(model[word].values())\n",
        "            model_count[word] = total_next_words\n",
        "            for word_next in model[word]:\n",
        "                model[word][word_next] = max(model[word][word_next] - D, 0) / total_next_words + (D / total_next_words * S[word] * unigram_model[word_next])\n",
        "\n",
        "        return model, model_count, D, S\n",
        "\n",
        "    def calculate_DS(self, model):\n",
        "        '''\n",
        "        Calculate the D and S of the model\n",
        "        '''\n",
        "        n1 = 0\n",
        "        n2 = 0\n",
        "        S = {}\n",
        "        for word in model:\n",
        "            S[word] = len(model[word])\n",
        "            for next_word in model[word]:\n",
        "                if model[word][next_word] == 1:\n",
        "                    n1 += 1\n",
        "                elif model[word][next_word] == 2:\n",
        "                    n2 += 1\n",
        "\n",
        "        D = n1 / (n1 + 2 * n2)\n",
        "\n",
        "        return D, S\n",
        "\n",
        "    def get_unirgam(self):\n",
        "        '''\n",
        "        Get the smoothed unigram model for the corpus\n",
        "        '''\n",
        "        model = {}\n",
        "        total_counts = 0\n",
        "        for sentence in self.corpus:\n",
        "            for word in sentence:\n",
        "                if word != '<s>':\n",
        "                    total_counts += 1\n",
        "                    if word in model:\n",
        "                        model[word] += 1\n",
        "                    else:\n",
        "                        model[word] = 1\n",
        "\n",
        "        number_tokens = len(model.keys())\n",
        "\n",
        "        for word in model:\n",
        "            model[word] = (model[word] + 1) / (total_counts + number_tokens)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def getSentenceLogProbability(self, sentence):\n",
        "        '''\n",
        "        Calculate the conditonal log probability of the sentence\n",
        "        Input is the list of tokens\n",
        "        Output is the float number that is the log probability of the sentence\n",
        "        '''\n",
        "        import math\n",
        "        D = self.D\n",
        "        S = self.S\n",
        "        unigram = self.unigram\n",
        "        log_prob_sum = 0\n",
        "        for i in range(len(sentence) - 1):\n",
        "            curr_word = sentence[i]\n",
        "            next_word = sentence[i+1]\n",
        "            total_next_words = self.model_count[curr_word]\n",
        "            if curr_word in self.model and next_word in self.model[curr_word]:\n",
        "                log_prob_sum += math.log(self.model[curr_word][next_word])\n",
        "            else:\n",
        "                log_prob_sum += math.log(D / total_next_words * S[curr_word] * unigram[next_word])\n",
        "\n",
        "\n",
        "        return log_prob_sum\n"
      ],
      "metadata": {
        "id": "tKCNA1oqv-iT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheck('smoothed-bigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50ettX5fwEFA",
        "outputId": "0fa2bb2a-d027-4402-cfd1-aa648607377d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- TEST: generateSentence() ---\n",
            "Test generateSentence() passed!\n",
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -16.355820202 \tYour log prob.: -16.355820202 \t PASSED \t ['<s>', 'Sonic', 'was', 'difficult', '.', '</s>']\n",
            "Correct log prob.: -76.0026113319 \tYour log prob.: -76.0026113319 \t PASSED \t ['<s>', 'By', 'the', 'Late', 'Classic', ',', 'a', 'network', 'of', 'few', '<UNK>', '(', 'few', '<UNK>', ')', 'linked', 'various', 'parts', 'of', 'the', 'city', ',', 'running', 'for', 'several', 'kilometres', 'through', 'its', 'urban', 'core', '.', '</s>']\n",
            "Correct log prob.: -74.2346475108 \tYour log prob.: -74.2346475108 \t PASSED \t ['<s>', 'Few', 'people', 'realize', 'how', 'difficult', 'it', 'was', 'to', 'create', 'Sonic', \"'s\", 'graphics', 'engine', ',', 'which', 'allowed', 'for', 'the', 'incredible', 'rate', 'of', 'speed', 'the', 'game', \"'s\", 'known', 'for', '.', '</s>']\n",
            "Correct log prob.: -47.2885760372 \tYour log prob.: -47.2885760372 \t PASSED \t ['<s>', 'Classic', 'few', 'parts', 'of', 'the', 'game', 'allowed', 'for', 'few', '<UNK>', '<UNK>', 'incredible', 'city', '.', '</s>']\n",
            "Correct log prob.: -51.2730261907 \tYour log prob.: -51.2730261907 \t PASSED \t ['<s>', 'Few', '<UNK>', 'realize', 'the', 'difficult', 'network', ',', 'which', 'linked', 'the', 'game', 'to', 'Sonic', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct train perp.: 12.2307627397 \tYour train perp.: 12.2307627397 \t PASSED\n",
            "Correct test perp.: 26.7193157699 \tYour test perp.: 26.7193157699 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    sanityCheckFullDataset('smoothed-bigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcJuCKYZwHn4",
        "outputId": "cc66223a-e6d7-43f1-cb82-a34e8a778409"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- TEST: getSentenceLogProbability(...) ---\n",
            "Correct log prob.: -61.3754065648 \tYour log prob.: -61.3754065648 \t PASSED \t ['<s>', 'He', 'was', '<UNK>', 'at', '<UNK>', 'College', ',', 'Hobart', ',', 'and', '<UNK>', 'in', '1932', '.', '</s>']\n",
            "Correct log prob.: -141.9754903887 \tYour log prob.: -141.9754903887 \t PASSED \t ['<s>', 'Despite', 'being', 'a', 'rare', 'Grade', '9', 'player', 'on', 'the', 'senior', 'team', ',', 'he', 'was', 'one', 'of', 'the', 'Knights', \"'\", 'two', 'leading', 'rushers', 'that', 'year', '.', '</s>']\n",
            "Correct log prob.: -107.0849366076 \tYour log prob.: -107.0849366076 \t PASSED \t ['<s>', 'Burke', \"'s\", 'total', 'was', 'a', 'school', 'record', 'for', 'the', 'Big', 'Ten', 'Conference', 'Men', \"'s\", 'Basketball', 'Tournament', '.', '</s>']\n",
            "Correct log prob.: -168.4944718788 \tYour log prob.: -168.4944718788 \t PASSED \t ['<s>', 'The', 'route', 'turns', 'to', 'the', 'northeast', ',', 'passing', 'near', 'the', '<UNK>', 'Leaf', 'Lakes', 'residential', 'development', ',', 'before', 'coming', 'to', 'an', 'interchange', 'with', 'US', '322', '(', 'Black', 'Horse', 'Pike', ')', '.', '</s>']\n",
            "Correct log prob.: -619.9409055374 \tYour log prob.: -619.9409055374 \t PASSED \t ['<s>', 'Two', 'points', 'are', 'contested', ':', 'first', ',', 'whether', 'or', 'not', 'the', 'teachings', 'of', 'Scientology', 'qualify', 'as', 'a', '\"', 'religion', 'or', '<UNK>', '\"', '(', 'Religion', 'or', '<UNK>', ';', 'these', 'are', 'equal', 'before', 'German', 'law', ')', ',', 'and', '<UNK>', ',', 'whether', 'or', 'not', 'these', 'teachings', 'are', 'only', 'used', 'as', 'a', 'pretext', 'for', 'purely', 'commercial', 'activity', ';', 'if', 'the', 'latter', 'were', 'the', 'case', ',', 'this', 'would', 'most', 'likely', 'imply', 'that', 'Scientology', 'would', 'not', 'qualify', 'for', 'protection', 'as', 'a', '\"', 'religious', 'or', '<UNK>', 'community', '\"', '(', '<UNK>', 'oder', '<UNK>', ')', 'under', 'Article', '4', 'of', 'the', 'German', 'constitution', ',', 'which', 'guarantees', 'the', 'freedom', 'of', 'belief', ',', 'religion', 'and', '<UNK>', '.', '</s>']\n",
            "Correct log prob.: -195.8159911677 \tYour log prob.: -195.8159911677 \t PASSED \t ['<s>', 'He', 'immediately', 'ran', 'into', 'a', 'problem', ':', 'the', 'South', 'Carolina', 'troops', '(', 'militia', 'or', 'the', 'colonial', 'regiments', ')', 'were', 'not', 'on', 'the', 'Continental', 'line', ',', 'and', 'thus', 'not', 'formally', 'under', 'his', 'authority', '.', '</s>']\n",
            "Correct log prob.: -86.3762008156 \tYour log prob.: -86.3762008156 \t PASSED \t ['<s>', 'One', 'of', 'them', 'was', 'a', 'bodyguard', 'who', 'was', 'present', 'at', 'the', 'concert', 'but', 'did', 'not', 'see', 'the', 'fall', '.', '</s>']\n",
            "Correct log prob.: -32.4764801981 \tYour log prob.: -32.4764801981 \t PASSED \t ['<s>', '<UNK>', 'was', 'relieved', 'on', '17', 'May', '.', '</s>']\n",
            "Correct log prob.: -48.124714509 \tYour log prob.: -48.124714509 \t PASSED \t ['<s>', 'US', 'Off', 'The', 'Planet', '!', '</s>']\n",
            "Correct log prob.: -124.687107856 \tYour log prob.: -124.687107856 \t PASSED \t ['<s>', 'The', 'difficulty', 'stems', 'from', 'the', 'relative', 'over', '@-@', 'stabilization', 'of', 'the', '<UNK>', 'cation', 'by', 'electron', 'donation', ',', '<UNK>', '<UNK>', '.', '</s>']\n",
            "Test getSentenceProbability(...) passed!\n",
            "\n",
            "--- TEST: getCorpusPerplexity(...) ---\n",
            "Correct test perp.: 261.4247123506 \tYour test perp.: 261.4247123506 \t PASSED\n",
            "Test getCorpusPerplexity(...) passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    runModel('smoothed-bigram')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7eJd8kswJKj",
        "outputId": "518025e3-3de5-4990-dbf9-23ffd49316e4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------- 5 sentences from your model ---------\n",
            "Log Probability: -194.67978113056623 \tSentence: ['<s>', 'Despite', 'being', 'transferred', 'to', 'the', 'world', 'around', 'him', 'and', 'the', 'nearby', 'and', 'his', 'historical', 'periods', 'of', 'Oldham', ',', 'to', 'the', 'penis', ')', 'is', 'often', \"'\", '\"', ',', 'one', 'of', 'the', 'MGB', 'turned', 'down', 'before', 'being', 'overly', 'dramatic', 'swing', ',', '\"', 'it', 'has', 'the', '1970s', '.', '</s>']\n",
            "Log Probability: -78.16105380040541 \tSentence: ['<s>', 'The', 'series', 'of', '2006', ',', 'overly', 'sentimental', 'and', 'under', '20', ',', 'they', 'supported', 'by', 'Jose', '<UNK>', '.', '</s>']\n",
            "Log Probability: -190.33510793872836 \tSentence: ['<s>', 'Baby', 'Just', 'as', 'she', 'never', 'be', 'traveling', 'over', 'to', 'begin', 'until', 'her', 'children', 'as', 'a', 'sequence', 'and', ',', 'resulting', 'in', 'the', 'previous', 'location', 'of', 'World', 'War', ',', '<UNK>', 'all', 'over', 'the', 'east', 'along', 'the', 'advantage', 'of', 'Leinster', ',', 'and', 'the', 'cast', 'in', 'the', '</s>']\n",
            "Log Probability: -41.34495907167674 \tSentence: ['<s>', 'The', 'group', 'of', 'slowly', 'being', 'featured', 'performer', '.', '</s>']\n",
            "Log Probability: -63.554860701791355 \tSentence: ['<s>', 'Europium', 'metal', 'in', 'the', 'Capel', 'Llanilltern', '–', '4', ',', 'and', 'storage', ',', 'and', 'Nuskhuri', '.', '</s>']\n",
            "\n",
            "--------- Corpus Perplexities ---------\n",
            "Training Set: 98.5581292053259\n",
            "Testing Set: 272.57201979320354\n"
          ]
        }
      ]
    }
  ]
}